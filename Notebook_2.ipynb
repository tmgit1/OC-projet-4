{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean price of orders: 120.65373901464716\n",
      "Minimum price of orders: 0.85\n",
      "Maximum price of orders: 6735.0\n",
      "payment_type\n",
      "credit_card    76795\n",
      "boleto         19784\n",
      "voucher         5775\n",
      "debit_card      1529\n",
      "not_defined        3\n",
      "Name: count, dtype: int64\n",
      "Date la plus ancienne : 2016-09-04 21:15:19\n",
      "Date la plus récente : 2018-10-17 17:30:18\n",
      "Nombre de valeurs manquantes par colonne :\n",
      "index_orders                             0\n",
      "order_id                                 0\n",
      "customer_id                              0\n",
      "order_status                             0\n",
      "order_purchase_timestamp                 0\n",
      "order_approved_at                      171\n",
      "order_delivered_carrier_date          1861\n",
      "order_delivered_customer_date         3030\n",
      "order_estimated_delivery_date            0\n",
      "index_customers                          0\n",
      "customer_unique_id                       0\n",
      "customer_zip_code_prefix                 0\n",
      "customer_city                            0\n",
      "customer_state                           0\n",
      "index_merged                             0\n",
      "payment_sequential                       0\n",
      "payment_type                             0\n",
      "payment_installments                     0\n",
      "payment_value                            0\n",
      "index_order_reviews                      0\n",
      "review_id                                0\n",
      "review_score                             0\n",
      "review_comment_title                 91681\n",
      "review_comment_message               60862\n",
      "review_creation_date                     0\n",
      "review_answer_timestamp                  0\n",
      "order_purchase_timestamp_datetime        0\n",
      "dtype: int64\n",
      "Nombre de valeurs manquantes par colonne :\n",
      "index_orders                         0\n",
      "order_id                             0\n",
      "customer_id                          0\n",
      "order_status                         0\n",
      "order_purchase_timestamp             0\n",
      "order_approved_at                    0\n",
      "order_delivered_carrier_date         0\n",
      "order_delivered_customer_date        0\n",
      "order_estimated_delivery_date        0\n",
      "index_customers                      0\n",
      "customer_unique_id                   0\n",
      "customer_zip_code_prefix             0\n",
      "customer_city                        0\n",
      "customer_state                       0\n",
      "index_merged                         0\n",
      "payment_sequential                   0\n",
      "payment_type                         0\n",
      "payment_installments                 0\n",
      "payment_value                        0\n",
      "index_order_reviews                  0\n",
      "review_id                            0\n",
      "review_score                         0\n",
      "review_creation_date                 0\n",
      "review_answer_timestamp              0\n",
      "order_purchase_timestamp_datetime    0\n",
      "dtype: int64\n",
      "Date la plus ancienne : 2016-10-03 09:44:50\n",
      "Date la plus récente : 2018-08-29 15:00:37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xt/s64lptzd6js02z9ywj1ydl8m0000gn/T/ipykernel_43366/608484631.py:100: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_df_cleaned.sort_values(by='order_purchase_timestamp_datetime', inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": "   frequency   recency  average_spending_per_order  review_score\n0   5.037118 -1.154033                   -0.529844      0.468876\n1  -0.212799  2.989249                   -0.560384     -0.119798\n2  -0.212799  2.989249                   -0.576413     -2.474491\n3  -0.212799  2.989249                   -0.492948     -0.904695\n4  -0.212799  2.989249                   -0.125691     -2.474491",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>frequency</th>\n      <th>recency</th>\n      <th>average_spending_per_order</th>\n      <th>review_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.037118</td>\n      <td>-1.154033</td>\n      <td>-0.529844</td>\n      <td>0.468876</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.212799</td>\n      <td>2.989249</td>\n      <td>-0.560384</td>\n      <td>-0.119798</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.212799</td>\n      <td>2.989249</td>\n      <td>-0.576413</td>\n      <td>-2.474491</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.212799</td>\n      <td>2.989249</td>\n      <td>-0.492948</td>\n      <td>-0.904695</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.212799</td>\n      <td>2.989249</td>\n      <td>-0.125691</td>\n      <td>-2.474491</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Les visualisations contenues dans ce notebook ont été commentés car elles alourdiraient trop le fichier ce qui rendrait son upload sur le repertoire Github impossible\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import scipy.stats as st\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Connection à la base de données SQLite\n",
    "conn = sqlite3.connect(\"olist.db\")\n",
    "\n",
    "# Sélectionne toutes les colonnes de la table \"orders\"\n",
    "query = \"SELECT * FROM orders;\"\n",
    "\n",
    "# Crée un dataframe avec la table sélectionnée précédemment\n",
    "df_orders = pd.read_sql_query(query, conn)\n",
    "\n",
    "query = \"SELECT * FROM customers;\"\n",
    "df_customers = pd.read_sql_query(query, conn)\n",
    "\n",
    "query = \"SELECT * FROM geoloc;\"\n",
    "df_geoloc = pd.read_sql_query(query, conn)\n",
    "\n",
    "query = \"SELECT * FROM order_items;\"\n",
    "df_order_items = pd.read_sql_query(query, conn)\n",
    "\n",
    "query = \"SELECT * FROM order_pymts;\"\n",
    "df_order_pymts = pd.read_sql_query(query, conn)\n",
    "\n",
    "query = \"SELECT * FROM order_reviews;\"\n",
    "df_order_reviews = pd.read_sql_query(query, conn)\n",
    "\n",
    "query = \"SELECT * FROM products;\"\n",
    "df_products = pd.read_sql_query(query, conn)\n",
    "\n",
    "query = \"SELECT * FROM sellers;\"\n",
    "df_sellers = pd.read_sql_query(query, conn)\n",
    "\n",
    "query = \"SELECT * FROM translation;\"\n",
    "df_translation = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Ferme la connection avec la base de données\n",
    "conn.close()\n",
    "# Analyse prix des commandes\n",
    "mean_price_df_order_items = df_order_items[\"price\"].mean()\n",
    "min_price_df_order_items = df_order_items[\"price\"].min()\n",
    "max_price_df_order_items = df_order_items[\"price\"].max()\n",
    "\n",
    "print(f\"Mean price of orders: {mean_price_df_order_items}\")\n",
    "print(f\"Minimum price of orders: {min_price_df_order_items}\")\n",
    "print(f\"Maximum price of orders: {max_price_df_order_items}\")\n",
    "# Analyse type de paiement\n",
    "payment_type_counts = df_order_pymts['payment_type'].value_counts()\n",
    "\n",
    "print(payment_type_counts)\n",
    "# Création du dataframe avec les colonnes utiles\n",
    "# Fusion de df_orders et df_customers sur la colonne \"customer_id\"\n",
    "merged_df = pd.merge(df_orders, df_customers, on='customer_id', how='inner', suffixes=('_orders', '_customers'))\n",
    "\n",
    "# Fusion du résultat avec df_order_pymts sur la colonne \"order_id\"\n",
    "merged_df = pd.merge(merged_df, df_order_pymts, on='order_id', how='inner', suffixes=('_merged', '_order_pymts'))\n",
    "\n",
    "# Fusion du résultat avec df_order_reviews sur la colonne \"order_id\"\n",
    "merged_df = pd.merge(merged_df, df_order_reviews, on='order_id', suffixes=('_merged', '_order_reviews'))\n",
    "\n",
    "merged_df.head()\n",
    "# Création d'une nouvelle colonne \"order_purchase_timestamp_datetime\" de type datetime\n",
    "merged_df['order_purchase_timestamp_datetime'] = pd.to_datetime(merged_df['order_purchase_timestamp'])\n",
    "merged_df.head()\n",
    "# Date la plus ancienne et date la plus récente\n",
    "earliest_date = merged_df['order_purchase_timestamp_datetime'].min()\n",
    "latest_date = merged_df['order_purchase_timestamp_datetime'].max()\n",
    "\n",
    "print(f\"Date la plus ancienne : {earliest_date}\")\n",
    "print(f\"Date la plus récente : {latest_date}\")\n",
    "missing_values_count = merged_df.isnull().sum()\n",
    "print(\"Nombre de valeurs manquantes par colonne :\")\n",
    "print(missing_values_count)\n",
    "# Suppression des colonnes \"review_comment_title\" et \"review_comment_message\"\n",
    "merged_df = merged_df.drop(['review_comment_title', 'review_comment_message'], axis=1)\n",
    "# Suppression des lignes avec des valeurs manquantes\n",
    "merged_df_cleaned = merged_df.dropna()\n",
    "missing_values_count = merged_df_cleaned.isnull().sum()\n",
    "print(\"Nombre de valeurs manquantes par colonne :\")\n",
    "print(missing_values_count)\n",
    "# Date la plus ancienne et date la plus récente\n",
    "earliest_date = merged_df_cleaned['order_purchase_timestamp_datetime'].min()\n",
    "latest_date = merged_df_cleaned['order_purchase_timestamp_datetime'].max()\n",
    "\n",
    "print(f\"Date la plus ancienne : {earliest_date}\")\n",
    "print(f\"Date la plus récente : {latest_date}\")\n",
    "# Trier le dataframe en fonction de la colonne 'order_purchase_timestamp_datetime'\n",
    "merged_df_cleaned.sort_values(by='order_purchase_timestamp_datetime', inplace=True)\n",
    "merged_df_cleaned.head()\n",
    "\n",
    "\n",
    "def creation_analyse_rfm(dataframe_periode, end_date):\n",
    "    \"\"\" Fonction permettant la réalisation d'une analyse rfm + average review score\n",
    "    dataframe_periode : dataframe sur lequel on souhaite réaliser l'analyse rfm\n",
    "    end_date : date de la fin de la période sur laquelle s'étend le dataframe\"\"\"\n",
    "\n",
    "    # Pour la récence\n",
    "    # Trouver la date de la commande la plus récente pour chaque customer_unique_id\n",
    "    latest_order_date_df = dataframe_periode.groupby('customer_unique_id')[\n",
    "        'order_purchase_timestamp_datetime'].max().reset_index()\n",
    "    latest_order_date_df.columns = ['customer_unique_id', 'latest_order_date']\n",
    "\n",
    "    # Calculer la récence en jours jusqu'à la fin de la période spécifiée\n",
    "    latest_order_date_df['recency'] = (pd.to_datetime(end_date) - latest_order_date_df['latest_order_date']).dt.days\n",
    "\n",
    "    # Pour la fréquence\n",
    "    # Calculer la fréquence d'achat par client\n",
    "    frequency_df = dataframe_periode.groupby('customer_unique_id')['order_id'].nunique().reset_index()\n",
    "    frequency_df.columns = ['customer_unique_id', 'frequency']\n",
    "\n",
    "    # Pour le montant\n",
    "    # Calculer le montant total dépensé par commande pour chaque client_unique_id\n",
    "    total_amount_spent = dataframe_periode.groupby('customer_unique_id')['payment_value'].sum()\n",
    "\n",
    "    # Calculer le nombre total de commandes pour chaque client_unique_id\n",
    "    total_orders = dataframe_periode.groupby('customer_unique_id')['order_id'].nunique()\n",
    "\n",
    "    # Calculer la moyenne par commande pour chaque client_unique_id\n",
    "    average_spending_per_order = total_amount_spent / total_orders\n",
    "\n",
    "    # Créer un DataFrame avec les résultats\n",
    "    average_spending_df = pd.DataFrame({'customer_unique_id': average_spending_per_order.index,\n",
    "                                        'average_spending_per_order': average_spending_per_order.values})\n",
    "\n",
    "    # Pour la moyenne du review_score par client\n",
    "    average_score_per_customer = merged_df.groupby('customer_unique_id')['review_score'].mean().reset_index()\n",
    "\n",
    "    # Fusion des DataFrames sur le customer_unique_id\n",
    "    rfm_df = pd.merge(frequency_df, latest_order_date_df, on='customer_unique_id', how='left')\n",
    "    rfm_df = pd.merge(rfm_df, average_spending_df, on='customer_unique_id', how='left')\n",
    "    rfm_df = pd.merge(rfm_df, average_score_per_customer, on='customer_unique_id', how='left')\n",
    "\n",
    "    # Ajout de la colonne 'order_purchase_timestamp_datetime'\n",
    "    rfm_df = pd.merge(rfm_df, dataframe_periode[['customer_unique_id', 'order_purchase_timestamp_datetime']],\n",
    "                      on='customer_unique_id', how='left')\n",
    "\n",
    "    # Trie du dataframe en fonction de la colonne 'order_purchase_timestamp_datetime'\n",
    "    rfm_df.sort_values(by='order_purchase_timestamp_datetime', inplace=True)\n",
    "\n",
    "    return rfm_df\n",
    "\n",
    "\n",
    "rfm_df_2016_2018 = creation_analyse_rfm(dataframe_periode=merged_df_cleaned, end_date=\"2018-10-31\")\n",
    "rfm_df_2016_2018.head()\n",
    "# Supprimer les colonnes non pertinentes\n",
    "columns_to_exclude = ['customer_unique_id', 'order_purchase_timestamp_datetime', \"latest_order_date\"]\n",
    "rfm_df_2016_2018_filtered = rfm_df_2016_2018.drop(columns=columns_to_exclude)\n",
    "\n",
    "rfm_df_2016_2018_filtered.head()\n",
    "# Créer un objet StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Standardisation de filtered_combined_df\n",
    "rfm_df_2016_2018_filtered_standardized = pd.DataFrame(scaler.fit_transform(rfm_df_2016_2018_filtered),\n",
    "                                                      columns=rfm_df_2016_2018_filtered.columns)\n",
    "\n",
    "rfm_df_2016_2018_filtered_standardized.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-09T18:45:52.648651Z",
     "start_time": "2024-09-09T18:45:37.371679Z"
    }
   },
   "id": "af6fe85f6ba18095",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-09T19:16:07.118625Z",
     "start_time": "2024-09-09T18:45:52.660542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparamètres: {'eps': 1.0, 'min_samples': 10}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Grille d'hyperparamètres\n",
    "param_grid = {\n",
    "    'eps': [0.1, 0.5, 1.0],\n",
    "    'min_samples': [5, 10, 20]\n",
    "}\n",
    "\n",
    "best_score = -1\n",
    "best_params = None\n",
    "\n",
    "# Gridsearch\n",
    "for params in ParameterGrid(param_grid):\n",
    "    dbscan = DBSCAN(**params)\n",
    "    dbscan.fit(rfm_df_2016_2018_filtered_standardized)\n",
    "    labels = dbscan.labels_\n",
    "    score = silhouette_score(rfm_df_2016_2018_filtered_standardized, labels)\n",
    "\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_params = params\n",
    "\n",
    "print(\"Hyperparamètres:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "   frequency   recency  average_spending_per_order  review_score\n0   5.037118 -1.154033                   -0.529844      0.468876\n1  -0.212799  2.989249                   -0.560384     -0.119798\n2  -0.212799  2.989249                   -0.576413     -2.474491\n3  -0.212799  2.989249                   -0.492948     -0.904695\n4  -0.212799  2.989249                   -0.125691     -2.474491",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>frequency</th>\n      <th>recency</th>\n      <th>average_spending_per_order</th>\n      <th>review_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.037118</td>\n      <td>-1.154033</td>\n      <td>-0.529844</td>\n      <td>0.468876</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.212799</td>\n      <td>2.989249</td>\n      <td>-0.560384</td>\n      <td>-0.119798</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.212799</td>\n      <td>2.989249</td>\n      <td>-0.576413</td>\n      <td>-2.474491</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.212799</td>\n      <td>2.989249</td>\n      <td>-0.492948</td>\n      <td>-0.904695</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.212799</td>\n      <td>2.989249</td>\n      <td>-0.125691</td>\n      <td>-2.474491</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entraînement d'un modèle DBSCAN\n",
    "\n",
    "# Créer un objet StandardScaler\n",
    "scaler_dbscan = StandardScaler()\n",
    "\n",
    "# Standardisation de filtered_combined_df\n",
    "rfm_df_2016_2018_filtered_standardized_dbscan = pd.DataFrame(scaler_dbscan.fit_transform(rfm_df_2016_2018_filtered), columns=rfm_df_2016_2018_filtered.columns)\n",
    "\n",
    "rfm_df_2016_2018_filtered_standardized_dbscan.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-09T19:16:07.360901Z",
     "start_time": "2024-09-09T19:16:07.146214Z"
    }
   },
   "id": "b3e615648cd7bd5b",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de silhouette : 0.4351535908387644\n"
     ]
    }
   ],
   "source": [
    "# Entraînement\n",
    "dbscan = DBSCAN(eps=1.0, min_samples=10)\n",
    "\n",
    "# Clustering\n",
    "cluster_label_dbscan = dbscan.fit_predict(rfm_df_2016_2018_filtered_standardized_dbscan)\n",
    "\n",
    "# Etiquettes de cluster\n",
    "rfm_df_2016_2018_filtered_standardized_dbscan['cluster_label_dbscan'] = cluster_label_dbscan\n",
    "\n",
    "# Calcul du score de silhouette\n",
    "silhouette_avg = silhouette_score(rfm_df_2016_2018_filtered_standardized_dbscan, cluster_label_dbscan)\n",
    "print(\"Score de silhouette :\", silhouette_avg)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-09T19:22:28.055375Z",
     "start_time": "2024-09-09T19:16:07.366275Z"
    }
   },
   "id": "da35ff787be84b57",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Déstandardisation du dataframe dbscan\n",
    "rfm_df_2016_2018_filtered_destandardized_dbscan = rfm_df_2016_2018_filtered_standardized_dbscan.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-09T19:22:28.075656Z",
     "start_time": "2024-09-09T19:22:28.060092Z"
    }
   },
   "id": "f82f4946a578073e",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Suppression de la colonne \"cluster_label\"\n",
    "rfm_df_2016_2018_filtered_destandardized_dbscan.drop('cluster_label_dbscan', axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-09T19:22:28.106750Z",
     "start_time": "2024-09-09T19:22:28.077643Z"
    }
   },
   "id": "6eff86712d944a5b",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Déstandardisation\n",
    "rfm_df_2016_2018_filtered_destandardized_dbscan = pd.DataFrame(\n",
    "    scaler_dbscan.inverse_transform(rfm_df_2016_2018_filtered_destandardized_dbscan),\n",
    "    columns=rfm_df_2016_2018_filtered_destandardized_dbscan.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-09T19:22:28.119450Z",
     "start_time": "2024-09-09T19:22:28.108513Z"
    }
   },
   "id": "da5933cfb615766",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "   frequency  recency  average_spending_per_order  review_score  \\\n0        3.0    124.0                       45.72          4.75   \n1        1.0    757.0                       39.09          4.00   \n2        1.0    757.0                       35.61          1.00   \n3        1.0    757.0                       53.73          3.00   \n4        1.0    757.0                      133.46          1.00   \n\n   cluster_label_dbscan  \n0                     0  \n1                     1  \n2                     1  \n3                     1  \n4                     1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>frequency</th>\n      <th>recency</th>\n      <th>average_spending_per_order</th>\n      <th>review_score</th>\n      <th>cluster_label_dbscan</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3.0</td>\n      <td>124.0</td>\n      <td>45.72</td>\n      <td>4.75</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>757.0</td>\n      <td>39.09</td>\n      <td>4.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>757.0</td>\n      <td>35.61</td>\n      <td>1.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>757.0</td>\n      <td>53.73</td>\n      <td>3.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>757.0</td>\n      <td>133.46</td>\n      <td>1.00</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ajout de la colonne 'cluster_label' au dataframe déstandardisé\n",
    "rfm_df_2016_2018_filtered_destandardized_dbscan['cluster_label_dbscan'] = rfm_df_2016_2018_filtered_standardized_dbscan[\n",
    "    'cluster_label_dbscan']\n",
    "rfm_df_2016_2018_filtered_destandardized_dbscan.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-09T19:22:28.161553Z",
     "start_time": "2024-09-09T19:22:28.123866Z"
    }
   },
   "id": "7a2d6ce52849d2db",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Visualisation clusters DBSCAN \"recency\" par rapport à \"average_spending_per_order\"\n",
    "# fig = px.scatter(rfm_df_2016_2018_filtered_destandardized_dbscan,\n",
    "#                  x='recency',\n",
    "#                  y='average_spending_per_order',\n",
    "#                  color=cluster_label_dbscan,\n",
    "#                  color_continuous_scale='viridis',\n",
    "#                  opacity=0.5,\n",
    "#                  labels={'recency': 'Recency', 'average_spending_per_order': 'Average Spending per Order'},\n",
    "#                  title='DBSCAN Clustering')\n",
    "\n",
    "# fig.update_layout(\n",
    "#     xaxis_title='Recency',\n",
    "#     yaxis_title='Average Spending per Order',\n",
    "#     title='DBSCAN Clustering'\n",
    "# )\n",
    "\n",
    "# fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-09T19:43:31.832978Z",
     "start_time": "2024-09-09T19:43:31.802233Z"
    }
   },
   "id": "c9f4e4e7ea3e5cc5",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Visualisation clusters DBSCAN \"frequency\" par rapport à \"average_spending_per_order\"\n",
    "# fig = px.scatter(rfm_df_2016_2018_filtered_destandardized_dbscan,\n",
    "#                  x='frequency',\n",
    "#                  y='average_spending_per_order',\n",
    "#                  color=cluster_label_dbscan,\n",
    "#                  color_continuous_scale='viridis',\n",
    "#                  opacity=0.5,\n",
    "#                  labels={'frequency': 'Frequency', 'average_spending_per_order': 'Average Spending per Order'},\n",
    "#                  title='DBSCAN Clustering')\n",
    "\n",
    "# fig.update_layout(\n",
    "#     xaxis_title='Frequency',\n",
    "#     yaxis_title='Average Spending per Order',\n",
    "#     title='DBSCAN Clustering'\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-09T19:43:48.137950Z",
     "start_time": "2024-09-09T19:43:48.131680Z"
    }
   },
   "id": "39bf4cb815306d25",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Visualisation clusters DBSCAN \"average_review_score\" par rapport à \"average_spending_per_order\"\n",
    "# fig = px.scatter(rfm_df_2016_2018_filtered_destandardized_dbscan,\n",
    "#                  x='review_score',\n",
    "#                  y='average_spending_per_order',\n",
    "#                  color=cluster_label_dbscan,\n",
    "#                  color_continuous_scale='viridis',\n",
    "#                  opacity=0.5,\n",
    "#                  labels={'review_score': 'Average Review Score',\n",
    "#                          'average_spending_per_order': 'Average Spending per Order'},\n",
    "#                  title='DBSCAN Clustering')\n",
    "\n",
    "# fig.update_layout(\n",
    "#     xaxis_title='Average Review Score',\n",
    "#     yaxis_title='Average Spending per Order',\n",
    "#     title='DBSCAN Clustering'\n",
    "# )\n",
    "\n",
    "# fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-09T21:09:50.095451Z",
     "start_time": "2024-09-09T21:09:50.067709Z"
    }
   },
   "id": "7c3ab7a0b91c929d",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Palette de couleurs personnalisée\n",
    "# custom_palette = sns.color_palette(\"husl\", n_colors=len(\n",
    "#     rfm_df_2016_2018_filtered_destandardized_dbscan['cluster_label_dbscan'].unique()))\n",
    "\n",
    "# Pairplot des caractéristiques\n",
    "# sns.set(style=\"ticks\")\n",
    "# sns.pairplot(rfm_df_2016_2018_filtered_destandardized_dbscan, hue='cluster_label_dbscan', diag_kind='kde', palette=custom_palette)\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-09T21:10:38.340465Z",
     "start_time": "2024-09-09T21:10:38.333364Z"
    }
   },
   "id": "a8d8decbd5243847",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Boxplot pour la variable \"recency\" du modèle DBSCAN\n",
    "# fig = px.box(rfm_df_2016_2018_filtered_destandardized_dbscan, x='cluster_label_dbscan', y='recency',\n",
    "#              color='cluster_label_dbscan', points='all', title='Boxplot de Recency par Cluster Modèle DBSCAN')\n",
    "# fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-09T21:11:13.756685Z",
     "start_time": "2024-09-09T21:11:13.746970Z"
    }
   },
   "id": "8fcffdb0e882b450",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Boxplot pour la variable \"frequency\" du modèle DBSCAN\n",
    "# fig = px.box(rfm_df_2016_2018_filtered_destandardized_dbscan, x='cluster_label_dbscan', y='frequency',\n",
    "#              color='cluster_label_dbscan', points='all', title='Boxplot de Frequency par Cluster Modèle DBSCAN')\n",
    "# fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-09T21:11:37.929969Z",
     "start_time": "2024-09-09T21:11:37.925107Z"
    }
   },
   "id": "1eff5cd064704385",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Boxplot pour la variable \"average_spending_per_order\" du modèle DBSCAN\n",
    "# fig = px.box(rfm_df_2016_2018_filtered_destandardized_dbscan, x='cluster_label_dbscan', y='average_spending_per_order',\n",
    "#              color='cluster_label_dbscan', points='all',\n",
    "#              title='Boxplot de average_spending_per_order par Cluster Modèle DBSCAN')\n",
    "# fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-09T21:12:00.578093Z",
     "start_time": "2024-09-09T21:12:00.572338Z"
    }
   },
   "id": "8c98bb4d38220bca",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Boxplot pour la variable average review score du modèle DBSCAN\n",
    "# fig = px.box(rfm_df_2016_2018_filtered_destandardized_dbscan, x='cluster_label_dbscan', y='review_score',\n",
    "#              color='cluster_label_dbscan', points='all',\n",
    "#              title='Boxplot de average review score par Cluster Modèle DBSCAN')\n",
    "# fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-09T21:12:39.931315Z",
     "start_time": "2024-09-09T21:12:39.918562Z"
    }
   },
   "id": "4d1e0a00543b396",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Partie maintenance du modèle\n",
    "def creation_data_frame_par_periode(start_date, end_date, merged_df_cleaned):\n",
    "    \"\"\"Crée un dataframe pour la période spécifiée :\n",
    "    start_date : début de la période souhaitée\n",
    "    end_date : fin de la période souhaitée\n",
    "     merge_df_cleaned = dataframe contenant les données sur lesquels on souhaite travailler\"\"\"\n",
    "\n",
    "    # Crée un dataframe pour la période spécifiée\n",
    "    filtered_df = merged_df_cleaned[(merged_df_cleaned['order_purchase_timestamp_datetime'] >= start_date) & (\n",
    "            merged_df_cleaned['order_purchase_timestamp_datetime'] <= end_date)].copy()\n",
    "\n",
    "    return filtered_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14890c8687a7cdbb",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def creation_analyse_rfm(dataframe_periode, end_date):\n",
    "    \"\"\"Fonction réalisant une analyse rfm + average review score sur le dataframe spécifié\n",
    "    dataframe_periode : dataframe de la période souhaitée sur lequel on souhaite réaliser l'analyse\n",
    "    end_date : dernier jour de la période souhaitée\n",
    "     \"\"\"\n",
    "    # Pour la récence\n",
    "    # Trouve la date de la commande la plus récente pour chaque customer_unique_id\n",
    "    latest_order_date_df = dataframe_periode.groupby('customer_unique_id')[\n",
    "        'order_purchase_timestamp_datetime'].max().reset_index()\n",
    "    latest_order_date_df.columns = ['customer_unique_id', 'latest_order_date']\n",
    "\n",
    "    # Calcul la récence en jours jusqu'à la fin de la période spécifiée\n",
    "    latest_order_date_df['recency'] = (pd.to_datetime(end_date) - latest_order_date_df['latest_order_date']).dt.days\n",
    "\n",
    "    # Pour la fréquence\n",
    "    # Calcul la fréquence d'achat par client\n",
    "    frequency_df = dataframe_periode.groupby('customer_unique_id')['order_id'].nunique().reset_index()\n",
    "    frequency_df.columns = ['customer_unique_id', 'frequency']\n",
    "\n",
    "    # Pour le montant\n",
    "    # Calcul le montant total dépensé par commande pour chaque client_unique_id\n",
    "    total_amount_spent = dataframe_periode.groupby('customer_unique_id')['payment_value'].sum()\n",
    "\n",
    "    # Calcul le nombre total de commandes pour chaque client_unique_id\n",
    "    total_orders = dataframe_periode.groupby('customer_unique_id')['order_id'].nunique()\n",
    "\n",
    "    # Calcul la moyenne par commande pour chaque client_unique_id\n",
    "    average_spending_per_order = total_amount_spent / total_orders\n",
    "\n",
    "    # Crée un DataFrame avec les résultats\n",
    "    average_spending_df = pd.DataFrame({'customer_unique_id': average_spending_per_order.index,\n",
    "                                        'average_spending_per_order': average_spending_per_order.values})\n",
    "\n",
    "    # Pour la moyenne du review_score par client\n",
    "    average_score_per_customer = merged_df.groupby('customer_unique_id')['review_score'].mean().reset_index()\n",
    "\n",
    "    # Fusion des DataFrames sur 'customer_unique_id'\n",
    "    rfm_df = pd.merge(frequency_df, latest_order_date_df, on='customer_unique_id', how='left')\n",
    "    rfm_df = pd.merge(rfm_df, average_spending_df, on='customer_unique_id', how='left')\n",
    "    rfm_df = pd.merge(rfm_df, average_score_per_customer, on='customer_unique_id', how='left')\n",
    "\n",
    "    # Ajoute la colonne 'order_purchase_timestamp_datetime'\n",
    "    rfm_df = pd.merge(rfm_df, dataframe_periode[['customer_unique_id', 'order_purchase_timestamp_datetime']],\n",
    "                      on='customer_unique_id', how='left')\n",
    "\n",
    "    # Trie le dataframe en fonction de la colonne 'order_purchase_timestamp_datetime'\n",
    "    rfm_df.sort_values(by='order_purchase_timestamp_datetime', inplace=True)\n",
    "\n",
    "    return rfm_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb8b076bb8167b61",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tom/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n",
      "\n",
      "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "\n",
      "/Users/tom/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n",
      "\n",
      "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "\n",
      "/Users/tom/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n",
      "\n",
      "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "\n",
      "/Users/tom/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n",
      "\n",
      "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "\n",
      "/Users/tom/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n",
      "\n",
      "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "\n",
      "/Users/tom/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n",
      "\n",
      "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "\n",
      "/Users/tom/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n",
      "\n",
      "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "\n",
      "/Users/tom/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n",
      "\n",
      "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "\n",
      "/Users/tom/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n",
      "\n",
      "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - Adjusted Rand Score entre le modèle de référence et le modèle 1: 1.0\n",
      "Iteration 2 - Adjusted Rand Score entre le modèle de référence et le modèle 2: 0.9140863725383442\n",
      "Iteration 3 - Adjusted Rand Score entre le modèle de référence et le modèle 3: 0.7282948230659595\n",
      "Iteration 4 - Adjusted Rand Score entre le modèle de référence et le modèle 4: 0.7328617472435557\n",
      "Iteration 5 - Adjusted Rand Score entre le modèle de référence et le modèle 5: 0.5665507808208012\n",
      "Iteration 6 - Adjusted Rand Score entre le modèle de référence et le modèle 6: 0.47906798686757285\n",
      "Iteration 7 - Adjusted Rand Score entre le modèle de référence et le modèle 7: 0.40247458415812654\n",
      "Iteration 8 - Adjusted Rand Score entre le modèle de référence et le modèle 8: 0.3334101815987905\n"
     ]
    }
   ],
   "source": [
    "# Maintenance\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "# Date d'initialisation\n",
    "init_date = datetime(2018, 1, 1)\n",
    "\n",
    "# Création du dataframe pour les données de 2016 à 2017\n",
    "df_1 = creation_data_frame_par_periode(start_date=\"2015-01-01\", end_date=init_date, merged_df_cleaned=merged_df_cleaned)\n",
    "\n",
    "# Création des analyses rfm pour le dataframe de référence\n",
    "rfm_1 = creation_analyse_rfm(dataframe_periode=df_1, end_date=init_date)\n",
    "\n",
    "# Suppression des colonnes non pertinentes\n",
    "columns_to_exclude = ['customer_unique_id', \"latest_order_date\", \"order_purchase_timestamp_datetime\"]\n",
    "\n",
    "# Entraînement du modèle Kmeans pour le dataframe de référence\n",
    "kmeans_1 = KMeans(n_clusters=5, random_state=42).fit(rfm_1.drop(columns=columns_to_exclude))\n",
    "\n",
    "# Initialisation de la liste pour stocker les scores ARI\n",
    "ari_scores = []\n",
    "\n",
    "# Boucle créant un dataframe par mois jusqu'au mois d'août 2018 (date de la dernière commande), puis création des analyses rfm, entraînement des modèles et comparaison de la similarité des modèles de 2016-2017\n",
    "for i in range(8):\n",
    "    end_date = init_date + timedelta(days=i * 30)\n",
    "    df_2 = creation_data_frame_par_periode(start_date=\"2015-01-01\", end_date=end_date,\n",
    "                                           merged_df_cleaned=merged_df_cleaned)\n",
    "\n",
    "    # Création des analyses rfm pour le dataframe actuel\n",
    "    rfm_2 = creation_analyse_rfm(dataframe_periode=df_2, end_date=end_date)\n",
    "\n",
    "    # Entraînement du modèle Kmeans pour le dataframe actuel\n",
    "    kmeans_2 = KMeans(n_clusters=5, random_state=42).fit(rfm_2.drop(columns=columns_to_exclude))\n",
    "\n",
    "    # Prédictions du modèle sur les nouvelles données\n",
    "    predict_reference = kmeans_1.predict(rfm_2.drop(columns=columns_to_exclude))\n",
    "    predict_2 = kmeans_2.predict(rfm_2.drop(columns=columns_to_exclude))\n",
    "\n",
    "    # Comparaison des modèles grâce à l'Adjusted Rand Score et stockage du score dans la liste\n",
    "    ari = adjusted_rand_score(predict_reference, predict_2)\n",
    "    ari_scores.append(ari)\n",
    "\n",
    "# Affichage des scores ARI pour chaque itération\n",
    "for i, ari in enumerate(ari_scores):\n",
    "    print(f\"Iteration {i + 1} - Adjusted Rand Score entre le modèle de référence et le modèle {i + 1}: {ari}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-09T19:23:58.865610Z",
     "start_time": "2024-09-09T19:23:42.809725Z"
    }
   },
   "id": "98b5c02367ae2efa",
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
